{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e47ba795-612a-40ae-bcbd-3a86ad23ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "import pytorch3d\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
    "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    DirectionalLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    TexturesUV,\n",
    "    TexturesVertex\n",
    ")\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d7a60f6-a145-4455-a5b0-7ea6639af047",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "    R=torch.eye(3).unsqueeze(0),\n",
    "    T=torch.tensor([[0, 0, 3]]),\n",
    "    fov=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4e372f9-22bc-4bb0-bc8c-2216b5a9afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3671573-be6a-4e8c-a2e9-0506e97d4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lights = PointLights(device=device, location=[[0.0, 0.0, 3.0]])\n",
    "from pytorch3d.renderer import PointLights\n",
    "\n",
    "# Key Light: Front-right, slightly above\n",
    "key_light = [3.0, 3.0, 2.0]\n",
    "\n",
    "# Fill Light: Front-left, lower intensity\n",
    "fill_light = [-3.0, 2.0, 2.0]\n",
    "\n",
    "# Back Light: Behind and slightly above the object\n",
    "back_light = [0.0, -3.0, 3.0]\n",
    "\n",
    "# Define the light colors (Key light is stronger, Fill light weaker)\n",
    "# Each color is in the form [R, G, B] with values between 0 and 1.\n",
    "ambient_color = [[0.1, 0.1, 0.1]] * 3   # Ambient light for all lights\n",
    "diffuse_color = [[1.0, 1.0, 1.0],       # Key light diffuse color\n",
    "                 [0.5, 0.5, 0.5],       # Fill light diffuse color\n",
    "                 [0.8, 0.8, 0.8]]       # Back light diffuse color\n",
    "specular_color = [[0.5, 0.5, 0.5]] * 3  # Specular highlights for all lights\n",
    "\n",
    "# Create the three-point lighting setup\n",
    "lights = PointLights(\n",
    "    location=[key_light, fill_light, back_light],\n",
    "    ambient_color=ambient_color,\n",
    "    diffuse_color=diffuse_color,\n",
    "    specular_color=specular_color\n",
    ")\n",
    "# Set up the renderer\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=512, \n",
    "    blur_radius=0.0, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device, \n",
    "        cameras=cameras,\n",
    "        lights=lights\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01a2953d-f068-45de-83a3-95fc02433efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the Python script \n",
    "url = \"https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save the content of the response as a Python file\n",
    "with open(\"plot_image_grid.py\", \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "from plot_image_grid import image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9e354f2-fdf3-43ae-9ce6-09e38f084962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = \"./data\"\n",
    "obj_filename = 'model.obj'\n",
    "\n",
    "\n",
    "# Load obj file\n",
    "mesh = load_objs_as_meshes([obj_filename], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39afbda8-b74a-498c-94a2-7648a53ee0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertices shape: torch.Size([1, 377, 3])\n",
      "Faces shape: torch.Size([1, 714, 3])\n",
      "Textures shape: torch.Size([1, 377, 3])\n"
     ]
    }
   ],
   "source": [
    "# Extract vertices, faces, and textures\n",
    "vertices = mesh.verts_packed()  # Shape: (N_v, 3)\n",
    "faces = mesh.faces_packed()      # Shape: (N_f, 3)\n",
    "# Extract textures, if available\n",
    "if mesh.textures is not None:\n",
    "    textures = mesh.textures.verts_rgb_packed()  # Shape: (N_v, 3)\n",
    "else:\n",
    "    # If no textures are available, you can create a dummy texture\n",
    "    textures = torch.ones_like(vertices)  # Create a white color for each vertex\n",
    "vertices = vertices.unsqueeze(0)  \n",
    "faces = faces.unsqueeze(0)          \n",
    "textures = textures.unsqueeze(0)    \n",
    "# Print the shapes of the extracted components\n",
    "print(\"Vertices shape:\", vertices.shape)  \n",
    "print(\"Faces shape:\", faces.shape)          \n",
    "print(\"Textures shape:\", textures.shape)    \n",
    "# Add a batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b29b5259-d564-4455-94a1-bfa679385c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "meshes = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fee3f87-e57d-4783-92a0-9614de751798",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2473901162496 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m R, T \u001b[38;5;241m=\u001b[39m look_at_view_transform(dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, elev\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, azim\u001b[38;5;241m=\u001b[39mangle\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     10\u001b[0m cameras \u001b[38;5;241m=\u001b[39m FoVPerspectiveCameras(R\u001b[38;5;241m=\u001b[39mR, T\u001b[38;5;241m=\u001b[39mT, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 12\u001b[0m rend \u001b[38;5;241m=\u001b[39m renderer(meshes, cameras\u001b[38;5;241m=\u001b[39mcameras, lights\u001b[38;5;241m=\u001b[39mlights)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Get the RGB image\u001b[39;00m\n\u001b[0;32m     15\u001b[0m image \u001b[38;5;241m=\u001b[39m rend[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Shape: (H, W, 3)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch3d\\renderer\\mesh\\renderer.py:64\u001b[0m, in \u001b[0;36mMeshRenderer.forward\u001b[1;34m(self, meshes_world, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03mRender a batch of images from a batch of meshes by rasterizing and then\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mshading.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mFor this set rasterizer.raster_settings.clip_barycentric_coords=True\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m fragments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrasterizer(meshes_world, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 64\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshader(fragments, meshes_world, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch3d\\renderer\\mesh\\shader.py:132\u001b[0m, in \u001b[0;36mSoftPhongShader.forward\u001b[1;34m(self, fragments, meshes, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m materials \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaterials\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaterials)\n\u001b[0;32m    131\u001b[0m blend_params \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblend_params\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblend_params)\n\u001b[1;32m--> 132\u001b[0m colors \u001b[38;5;241m=\u001b[39m phong_shading(\n\u001b[0;32m    133\u001b[0m     meshes\u001b[38;5;241m=\u001b[39mmeshes,\n\u001b[0;32m    134\u001b[0m     fragments\u001b[38;5;241m=\u001b[39mfragments,\n\u001b[0;32m    135\u001b[0m     texels\u001b[38;5;241m=\u001b[39mtexels,\n\u001b[0;32m    136\u001b[0m     lights\u001b[38;5;241m=\u001b[39mlights,\n\u001b[0;32m    137\u001b[0m     cameras\u001b[38;5;241m=\u001b[39mcameras,\n\u001b[0;32m    138\u001b[0m     materials\u001b[38;5;241m=\u001b[39mmaterials,\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    140\u001b[0m znear \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mznear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(cameras, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mznear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1.0\u001b[39m))\n\u001b[0;32m    141\u001b[0m zfar \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzfar\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(cameras, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzfar\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100.0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch3d\\renderer\\mesh\\shading.py:121\u001b[0m, in \u001b[0;36mphong_shading\u001b[1;34m(meshes, fragments, lights, cameras, materials, texels)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mphong_shading\u001b[39m(\n\u001b[0;32m    101\u001b[0m     meshes, fragments, lights, cameras, materials, texels\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    Apply per pixel shading. First interpolate the vertex normals and\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    vertex coordinates using the barycentric coordinates to get the position\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m        colors: (N, H, W, K, 3)\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     colors, _ \u001b[38;5;241m=\u001b[39m _phong_shading_with_pixels(\n\u001b[0;32m    122\u001b[0m         meshes, fragments, lights, cameras, materials, texels\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m colors\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch3d\\renderer\\mesh\\shading.py:93\u001b[0m, in \u001b[0;36m_phong_shading_with_pixels\u001b[1;34m(meshes, fragments, lights, cameras, materials, texels)\u001b[0m\n\u001b[0;32m     87\u001b[0m pixel_coords_in_camera \u001b[38;5;241m=\u001b[39m interpolate_face_attributes(\n\u001b[0;32m     88\u001b[0m     fragments\u001b[38;5;241m.\u001b[39mpix_to_face, fragments\u001b[38;5;241m.\u001b[39mbary_coords, faces_verts\n\u001b[0;32m     89\u001b[0m )\n\u001b[0;32m     90\u001b[0m pixel_normals \u001b[38;5;241m=\u001b[39m interpolate_face_attributes(\n\u001b[0;32m     91\u001b[0m     fragments\u001b[38;5;241m.\u001b[39mpix_to_face, fragments\u001b[38;5;241m.\u001b[39mbary_coords, faces_normals\n\u001b[0;32m     92\u001b[0m )\n\u001b[1;32m---> 93\u001b[0m ambient, diffuse, specular \u001b[38;5;241m=\u001b[39m _apply_lighting(\n\u001b[0;32m     94\u001b[0m     pixel_coords_in_camera, pixel_normals, lights, cameras, materials\n\u001b[0;32m     95\u001b[0m )\n\u001b[0;32m     96\u001b[0m colors \u001b[38;5;241m=\u001b[39m (ambient \u001b[38;5;241m+\u001b[39m diffuse) \u001b[38;5;241m*\u001b[39m texels \u001b[38;5;241m+\u001b[39m specular\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m colors, pixel_coords_in_camera\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch3d\\renderer\\mesh\\shading.py:35\u001b[0m, in \u001b[0;36m_apply_lighting\u001b[1;34m(points, normals, lights, cameras, materials)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    points: torch tensor of shape (N, ..., 3) or (P, 3).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    specular_color: same shape as the input points\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m light_diffuse \u001b[38;5;241m=\u001b[39m lights\u001b[38;5;241m.\u001b[39mdiffuse(normals\u001b[38;5;241m=\u001b[39mnormals, points\u001b[38;5;241m=\u001b[39mpoints)\n\u001b[1;32m---> 35\u001b[0m light_specular \u001b[38;5;241m=\u001b[39m lights\u001b[38;5;241m.\u001b[39mspecular(\n\u001b[0;32m     36\u001b[0m     normals\u001b[38;5;241m=\u001b[39mnormals,\n\u001b[0;32m     37\u001b[0m     points\u001b[38;5;241m=\u001b[39mpoints,\n\u001b[0;32m     38\u001b[0m     camera_position\u001b[38;5;241m=\u001b[39mcameras\u001b[38;5;241m.\u001b[39mget_camera_center(),\n\u001b[0;32m     39\u001b[0m     shininess\u001b[38;5;241m=\u001b[39mmaterials\u001b[38;5;241m.\u001b[39mshininess,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m ambient_color \u001b[38;5;241m=\u001b[39m materials\u001b[38;5;241m.\u001b[39mambient_color \u001b[38;5;241m*\u001b[39m lights\u001b[38;5;241m.\u001b[39mambient_color\n\u001b[0;32m     42\u001b[0m diffuse_color \u001b[38;5;241m=\u001b[39m materials\u001b[38;5;241m.\u001b[39mdiffuse_color \u001b[38;5;241m*\u001b[39m light_diffuse\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch3d\\renderer\\lighting.py:280\u001b[0m, in \u001b[0;36mPointLights.specular\u001b[1;34m(self, normals, points, camera_position, shininess)\u001b[0m\n\u001b[0;32m    278\u001b[0m location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_location(points)\n\u001b[0;32m    279\u001b[0m direction \u001b[38;5;241m=\u001b[39m location \u001b[38;5;241m-\u001b[39m points\n\u001b[1;32m--> 280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m specular(\n\u001b[0;32m    281\u001b[0m     points\u001b[38;5;241m=\u001b[39mpoints,\n\u001b[0;32m    282\u001b[0m     normals\u001b[38;5;241m=\u001b[39mnormals,\n\u001b[0;32m    283\u001b[0m     color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecular_color,\n\u001b[0;32m    284\u001b[0m     direction\u001b[38;5;241m=\u001b[39mdirection,\n\u001b[0;32m    285\u001b[0m     camera_position\u001b[38;5;241m=\u001b[39mcamera_position,\n\u001b[0;32m    286\u001b[0m     shininess\u001b[38;5;241m=\u001b[39mshininess,\n\u001b[0;32m    287\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pytorch3d\\renderer\\lighting.py:147\u001b[0m, in \u001b[0;36mspecular\u001b[1;34m(points, normals, direction, color, camera_position, shininess)\u001b[0m\n\u001b[0;32m    145\u001b[0m normals \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(normals, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m    146\u001b[0m direction \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(direction, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m--> 147\u001b[0m cos_angle \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(normals \u001b[38;5;241m*\u001b[39m direction, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# No specular highlights if angle is less than 0.\u001b[39;00m\n\u001b[0;32m    149\u001b[0m mask \u001b[38;5;241m=\u001b[39m (cos_angle \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2473901162496 bytes."
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "num_frames = 60  # Number of frames for the GIF\n",
    "angles = torch.linspace(0, 360, num_frames)  # Angles for full rotation\n",
    "images = []  # To store the rendered images\n",
    "\n",
    "for angle in angles:\n",
    "    # Compute the camera position and orientation\n",
    "    R, T = look_at_view_transform(dist=3, elev=5, azim=angle.item())\n",
    "    cameras = FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "\n",
    "    rend = renderer(meshes, cameras=cameras, lights=lights)\n",
    "    \n",
    "    # Get the RGB image\n",
    "    image = rend[0, ..., :3].cpu().numpy()  # Shape: (H, W, 3)\n",
    "    # Normalize the image to [0, 1] and convert to uint8\n",
    "    # Check for non-zero values before normalization\n",
    "    if image.max() > 0:  # Only normalize if there's a non-zero value\n",
    "        epsilon = 1e-8\n",
    "        image = (image - image.min()) / (image.max() - image.min() + epsilon)\n",
    "    \n",
    "    # Convert to uint8\n",
    "    image = (image * 255).astype(np.uint8)  # Convert to uint8\n",
    "    \n",
    "    images.append(image)\n",
    "\n",
    "# Save the images as a GIF\n",
    "imageio.mimsave('Pikachu_360_degree_render3.gif', images, fps=15)\n",
    "\n",
    "# Optional: Display the first image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(images[0])  # Display the first frame\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88058bff-701d-4c57-8252-371dd98c2575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12584546-fdf7-4f2b-af53-af873cb4b44f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
